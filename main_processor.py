#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»å¤„ç†ç¨‹åº
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
"""
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='joblib')

import json
from typing import Dict, Any
from text_preprocessing import TextPreprocessor
from lda_trainer import LDATrainer
from keyword_dictionary import KeywordDictionary
from data_mapper import DataMapper
from report_generator import ReportGenerator

class PorcelainProcessor:
    """ç“·å™¨æ•°æ®å¤„ç†ä¸»ç¨‹åº"""
    
    def __init__(self, keyword_config: str = None):
        # åˆå§‹åŒ–å„æ¨¡å—
        self.preprocessor = TextPreprocessor()
        self.lda_trainer = LDATrainer()
        self.keyword_dict = KeywordDictionary(keyword_config)
        self.mapper = DataMapper(self.keyword_dict)
        self.report_gen = ReportGenerator()
    
    def process_dataset(self, input_file: str, output_file: str, 
                       use_optimal_topics: bool = True) -> Dict[str, Any]:
        """å¤„ç†æ•´ä¸ªæ•°æ®é›†"""
        try:
            print(f"ğŸ“ åŠ è½½æ•°æ®: {input_file}")
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"ğŸ“Š æ•°æ®é‡: {len(data)} æ¡è®°å½•")
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
            return {}
        
        # 1. æ–‡æœ¬é¢„å¤„ç†
        print("\nğŸ” å‡†å¤‡æ–‡æœ¬æ•°æ®...")
        all_texts = []
        processed_texts = []
        
        for item in data:
            # æå–åŸå§‹æ–‡æœ¬
            raw_text = self.preprocessor.extract_text_from_item(item)
            all_texts.append(raw_text)
            
            # é¢„å¤„ç†æ–‡æœ¬
            processed_text = self.preprocessor.preprocess_text(raw_text)
            if processed_text:
                processed_texts.append(processed_text)
        
        print(f"æ€»æ–‡æœ¬æ•°: {len(all_texts)}")
        print(f"æœ‰æ•ˆå¤„ç†æ–‡æœ¬æ•°: {len(processed_texts)}")
        
        # 2. LDAè®­ç»ƒ
        if processed_texts:
            self.lda_trainer.train_lda(processed_texts, use_optimal_topics)
        
        # 3. æ•°æ®æ˜ å°„
        processed_data = []
        stats = self._init_stats()
        
        print("\nğŸ”„ å¼€å§‹æ•°æ®æ˜ å°„...")
        for i, item in enumerate(data):
            if i % 100 == 0:
                print(f"å¤„ç†è¿›åº¦: {i}/{len(data)} ({i/len(data)*100:.1f}%)")
            
            # è·å–æ–‡æœ¬
            raw_text = all_texts[i]
            processed_text = self.preprocessor.preprocess_text(raw_text)
            
            # è·å–LDAä¸»é¢˜
            lda_topics = None
            if self.lda_trainer.lda_model and processed_text:
                lda_topics = self.lda_trainer.get_document_topics(processed_text)
            
            # æ˜ å°„æ•°æ®
            result = self.mapper.process_item(item, raw_text, lda_topics)
            
            # æ›´æ–°ç»Ÿè®¡
            if 'error' not in result.get('ProcessingMetadata', {}):
                processed_data.append(result)
                self._update_stats(stats, result)
            else:
                stats['errors'] += 1
        
        # 4. ä¿å­˜ç»“æœ
        self._save_results(output_file, processed_data, stats)
        
        # 5. æ‰“å°ç»Ÿè®¡
        self._print_statistics(stats)
        
        return stats
    
    def _init_stats(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®"""
        return {
            'total': 0,
            'processed': 0,
            'with_decorations': 0,
            'with_shape': 0,
            'with_function': 0,
            'with_inscriptions': 0,
            'with_color': 0,
            'with_glaze': 0,
            'with_place': 0,
            'with_topics': 0,
            'high_quality': 0,
            'errors': 0
        }
    
    def _update_stats(self, stats: Dict[str, Any], result: Dict[str, Any]):
        """æ›´æ–°ç»Ÿè®¡æ•°æ®"""
        stats['processed'] += 1
        desc_meta = result.get('DescriptiveMetadata', {})
        
        if desc_meta.get('Decorations'):
            stats['with_decorations'] += 1
        if desc_meta.get('Shape'):
            stats['with_shape'] += 1
        if desc_meta.get('Function'):
            stats['with_function'] += 1
        if desc_meta.get('Inscriptions'):
            stats['with_inscriptions'] += 1
        if desc_meta.get('ColoredDrawing'):
            stats['with_color'] += 1
        if desc_meta.get('Glaze'):
            stats['with_glaze'] += 1
        if desc_meta.get('ProductionPlace'):
            stats['with_place'] += 1
        if desc_meta.get('lda_topics'):
            stats['with_topics'] += 1
        
        quality_score = desc_meta.get('quality_score', 0)
        if quality_score > 0.5:
            stats['high_quality'] += 1
    
    def _save_results(self, output_file: str, processed_data: list[Dict], 
                     stats: Dict[str, Any]):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        try:
            # ä¿å­˜ä¸»æ•°æ®
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats_file = output_file.replace('.json', '_stats.json')
            stats['total'] = len(processed_data) + stats['errors']
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
            
            # ä¿å­˜LDAä¿¡æ¯
            if self.lda_trainer.lda_model:
                lda_info = {
                    'n_topics': self.lda_trainer.n_topics,
                    'optimal_topics': self.lda_trainer.optimal_topics,
                    'vocabulary_size': len(self.lda_trainer.vectorizer.get_feature_names_out())
                        if self.lda_trainer.vectorizer else 0
                }
                lda_file = output_file.replace('.json', '_lda_info.json')
                with open(lda_file, 'w', encoding='utf-8') as f:
                    json.dump(lda_info, f, ensure_ascii=False, indent=2)
                print(f"ğŸ·ï¸  LDAä¿¡æ¯å·²ä¿å­˜åˆ°: {lda_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    def _print_statistics(self, stats: Dict[str, Any]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡æ€»è§ˆ:")
        print("=" * 80)
        print(f"ğŸ“ æ€»è®°å½•æ•°: {stats['total']}")
        print(f"âœ… æˆåŠŸå¤„ç†: {stats['processed']} ({stats['processed']/stats['total']*100:.1f}%)")
        if stats['errors'] > 0:
            print(f"âŒ å¤„ç†é”™è¯¯: {stats['errors']} ({stats['errors']/stats['total']*100:.1f}%)")
        
        print(f"\nğŸ“Š å­—æ®µæå–ç»Ÿè®¡:")
        print("-" * 80)
        fields = [
            ('è£…é¥°ä¿¡æ¯', 'with_decorations', 'ğŸ¨'),
            ('å½¢çŠ¶ä¿¡æ¯', 'with_shape', 'ğŸº'),
            ('åŠŸèƒ½ä¿¡æ¯', 'with_function', 'ğŸ”§'),
            ('æ¬¾è¯†ä¿¡æ¯', 'with_inscriptions', 'ğŸ“œ'),
            ('é¢œè‰²ä¿¡æ¯', 'with_color', 'ğŸ¨'),
            ('é‡‰è‰²ä¿¡æ¯', 'with_glaze', 'âœ¨'),
            ('äº§åœ°ä¿¡æ¯', 'with_place', 'ğŸ“'),
            ('ä¸»é¢˜ä¿¡æ¯', 'with_topics', 'ğŸ·ï¸')
        ]
        
        for field_name, field_key, icon in fields:
            count = stats.get(field_key, 0)
            percentage = count/stats['total']*100 if stats['total'] > 0 else 0
            bar_length = int(percentage / 5)
            bar = 'â–ˆ' * bar_length + 'â–‘' * (20 - bar_length)
            print(f"{icon} {field_name:12} {bar} {count:5d} ({percentage:5.1f}%)")
        
        print(f"\nğŸ† é«˜è´¨é‡è®°å½• (è´¨é‡åˆ†æ•°>0.5): {stats['high_quality']} ({stats['high_quality']/stats['total']*100:.1f}%)")
    
    def generate_report(self, data_file: str):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“ æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        self.report_gen.generate_analysis_report(data_file)


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    input_file = 'chinese_porcelain_metadata.json'
    output_file = 'porcelain_final_structured.json'
    keyword_config = None  # å¯é€‰çš„å…³é”®è¯é…ç½®æ–‡ä»¶
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = PorcelainProcessor(keyword_config)
    
    # å¤„ç†æ•°æ®
    print("ğŸš€ å¼€å§‹ç“·å™¨æ•°æ®å¤„ç†...")
    print("=" * 80)
    
    stats = processor.process_dataset(
        input_file=input_file,
        output_file=output_file,
        use_optimal_topics=True
    )
    
    print("\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼")
    
    # ç”ŸæˆæŠ¥å‘Š
    processor.generate_report(output_file)
    
    print("\nğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()