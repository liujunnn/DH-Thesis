#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LDAè®­ç»ƒæ¨¡å—
åŠŸèƒ½ï¼šLDAæ¨¡å‹è®­ç»ƒã€å›°æƒ‘åº¦åˆ†æã€å‚æ•°ä¼˜åŒ–
"""

import numpy as np
from typing import List, Dict, Tuple, Optional
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from collections import Counter

class LDATrainer:
    """LDAè®­ç»ƒå™¨"""
    
    def __init__(self, n_topics: Optional[int] = None):
        self.n_topics = n_topics
        self.lda_model = None
        self.vectorizer = None
        self.tfidf_vectorizer = None
        self.optimal_topics = None
        self.topic_labels = {}
    
    def find_optimal_topics(self, texts: List[str], 
                        min_topics: int = 5, 
                        max_topics: int = 20) -> int:
        """ä½¿ç”¨å›°æƒ‘åº¦æ‰¾åˆ°æœ€ä½³ä¸»é¢˜æ•°"""
        print(f"\nğŸ” å¯»æ‰¾æœ€ä½³ä¸»é¢˜æ•° (èŒƒå›´: {min_topics}-{max_topics})...")
        
        if len(texts) < 50:
            print("âš ï¸  æ–‡æœ¬æ•°é‡å¤ªå°‘ï¼Œä½¿ç”¨é»˜è®¤ä¸»é¢˜æ•°: 10")
            return 10
        
        # åˆ›å»ºè¯è¢‹æ¨¡å‹
        temp_vectorizer = CountVectorizer(
            max_df=0.7,
            min_df=5,
            max_features=500,
            ngram_range=(1, 2),
            token_pattern=r'\b[a-zA-Z]{3,}\b'
        )
        
        doc_term_matrix = temp_vectorizer.fit_transform(texts)
        
        # è®¡ç®—ä¸åŒä¸»é¢˜æ•°çš„å›°æƒ‘åº¦
        perplexities = []
        coherences = []
        topic_range = range(min_topics, max_topics + 1)
        
        for n_topic in topic_range:
            print(f"  æµ‹è¯•ä¸»é¢˜æ•°: {n_topic}", end='')
            
            # è®­ç»ƒLDAæ¨¡å‹
            lda = LatentDirichletAllocation(
                n_components=n_topic,
                random_state=42,
                max_iter=100,
                learning_method='batch',
                doc_topic_prior=0.05,  # alpha
                topic_word_prior=0.001,  # beta
                n_jobs=-1
            )
            
            lda.fit(doc_term_matrix)
            
            # è®¡ç®—å›°æƒ‘åº¦
            perplexity = lda.perplexity(doc_term_matrix)
            perplexities.append(perplexity)
            
            # è®¡ç®—ä¸»é¢˜ä¸€è‡´æ€§
            coherence = self._calculate_topic_coherence(lda, temp_vectorizer, texts)
            coherences.append(coherence)
            
            print(f" - å›°æƒ‘åº¦: {perplexity:.2f}, ä¸€è‡´æ€§: {coherence:.3f}")
        
        # æ‰¾åˆ°æœ€ä½³ä¸»é¢˜æ•°
        optimal_topics = self._find_elbow_point(topic_range, perplexities, coherences)
        
        print(f"\nâœ… æœ€ä½³ä¸»é¢˜æ•°: {optimal_topics}")
        
        return optimal_topics
    
    def _calculate_topic_coherence(self, lda_model, vectorizer, texts: List[str]) -> float:
        """è®¡ç®—ä¸»é¢˜ä¸€è‡´æ€§"""
        feature_names = vectorizer.get_feature_names_out()
        coherence_scores = []
        
        for topic_idx in range(lda_model.n_components):
            # è·å–ä¸»é¢˜ä¸­æ¦‚ç‡æœ€é«˜çš„10ä¸ªè¯
            top_word_indices = lda_model.components_[topic_idx].argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_word_indices]
            
            # è®¡ç®—è¯å¯¹å…±ç°
            word_pairs = [(top_words[i], top_words[j]) 
                         for i in range(len(top_words)) 
                         for j in range(i+1, len(top_words))]
            
            pair_scores = []
            for w1, w2 in word_pairs[:10]:
                co_occur = sum(1 for text in texts if w1 in text and w2 in text)
                occur_w1 = sum(1 for text in texts if w1 in text)
                
                if occur_w1 > 0:
                    score = co_occur / occur_w1
                    pair_scores.append(score)
            
            if pair_scores:
                coherence_scores.append(np.mean(pair_scores))
        
        return np.mean(coherence_scores) if coherence_scores else 0.0
    
    def _find_elbow_point(self, topic_range, perplexities, coherences) -> int:
        """æ‰¾åˆ°è‚˜éƒ¨ç‚¹ï¼ˆæœ€ä½³ä¸»é¢˜æ•°ï¼‰"""
        # æ ‡å‡†åŒ–æŒ‡æ ‡
        perp_normalized = (np.array(perplexities) - np.min(perplexities)) / (np.max(perplexities) - np.min(perplexities))
        coh_normalized = (np.array(coherences) - np.min(coherences)) / (np.max(coherences) - np.min(coherences))
        
        # ç»„åˆæŒ‡æ ‡
        combined_score = perp_normalized - coh_normalized
        
        # è®¡ç®—äºŒé˜¶å¯¼æ•°æ‰¾æ‹ç‚¹
        if len(combined_score) > 2:
            second_derivative = np.diff(np.diff(combined_score))
            elbow_index = np.argmax(second_derivative) + 2
            elbow_index = min(elbow_index, len(topic_range) - 1)
        else:
            elbow_index = len(topic_range) // 2
        
        return list(topic_range)[elbow_index]
    
    def train_lda(self, texts: List[str], use_optimal_topics: bool = True):
        """è®­ç»ƒLDAæ¨¡å‹"""
        print("\nğŸš€ è®­ç»ƒLDAæ¨¡å‹...")
        
        if len(texts) < 10:
            print("âš ï¸  æœ‰æ•ˆæ–‡æœ¬å¤ªå°‘ï¼Œè·³è¿‡LDAè®­ç»ƒ")
            return
        
        print(f"ğŸ“Š è®­ç»ƒæ–‡æœ¬æ•°: {len(texts)}")
        
        # ç¡®å®šä¸»é¢˜æ•°
        if use_optimal_topics and self.n_topics is None:
            self.n_topics = self.find_optimal_topics(texts, min_topics=8, max_topics=20)
        elif self.n_topics is None:
            self.n_topics = 15
        
        # åˆ›å»ºè¯è¢‹æ¨¡å‹
        self.vectorizer = CountVectorizer(
            max_df=0.7,
            min_df=5,
            max_features=500,
            ngram_range=(1, 2),
            token_pattern=r'\b[a-zA-Z]{3,}\b'
        )
        
        # åˆ›å»ºTF-IDFæ¨¡å‹
        self.tfidf_vectorizer = TfidfVectorizer(
            max_df=0.7,
            min_df=5,
            max_features=500,
            ngram_range=(1, 2),
            token_pattern=r'\b[a-zA-Z]{3,}\b'
        )
        
        try:
            # è®­ç»ƒè¯è¢‹æ¨¡å‹
            doc_term_matrix = self.vectorizer.fit_transform(texts)
            
            # æ£€æŸ¥è¯æ±‡è¡¨å¤§å°
            vocab_size = len(self.vectorizer.get_feature_names_out())
            print(f"ğŸ“– è¯æ±‡è¡¨å¤§å°: {vocab_size}")
            
            # è®­ç»ƒTF-IDFæ¨¡å‹
            self.tfidf_vectorizer.fit_transform(texts)
            
            # è®­ç»ƒLDAæ¨¡å‹
            self.lda_model = LatentDirichletAllocation(
                n_components=self.n_topics,
                random_state=42,
                max_iter=200,
                learning_method='batch',
                learning_offset=10.0,
                doc_topic_prior=0.05,  # alpha
                topic_word_prior=0.001,  # beta
                n_jobs=-1
            )
            
            self.lda_model.fit(doc_term_matrix)
            
            print("âœ… LDAæ¨¡å‹è®­ç»ƒå®Œæˆ")
            
            # æ‰“å°ä¸»é¢˜æ‘˜è¦
            print(f"\nğŸ“‹ ä¸»é¢˜æ‘˜è¦ ({self.n_topics} ä¸ªä¸»é¢˜):")
            for topic_id in range(self.n_topics):
                top_words = self.get_topic_words(topic_id, n_words=5)
                words = [word for word, _ in top_words]
                print(f"  ä¸»é¢˜ {topic_id + 1}: {', '.join(words)}")
            
        except Exception as e:
            print(f"âŒ LDAè®­ç»ƒå¤±è´¥: {e}")
            self.lda_model = None
    
    def get_document_topics(self, text: str) -> List[Dict]:
        """è·å–æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒ"""
        if not self.lda_model or not self.vectorizer:
            return []
        
        try:
            doc_vector = self.vectorizer.transform([text])
            
            if doc_vector.nnz > 0:
                topic_dist = self.lda_model.transform(doc_vector)[0]
                
                topics = []
                for idx, prob in enumerate(topic_dist):
                    if prob > 0.15:  # é˜ˆå€¼
                        topics.append({
                            'topic_id': idx,
                            'probability': float(prob)
                        })
                
                topics.sort(key=lambda x: x['probability'], reverse=True)
                return topics[:3]
                
        except:
            pass
        
        return []
    
    def get_topic_words(self, topic_id: int, n_words: int = 10) -> List[Tuple[str, float]]:
        """è·å–ä¸»é¢˜çš„å…³é”®è¯"""
        if not self.lda_model or not self.vectorizer:
            return []
        
        feature_names = self.vectorizer.get_feature_names_out()
        topic = self.lda_model.components_[topic_id]
        top_indices = topic.argsort()[-n_words:][::-1]
        
        words_weights = []
        for idx in top_indices:
            words_weights.append((feature_names[idx], topic[idx]))
        
        return words_weights
    
    def extract_keywords_tfidf(self, texts: List[str], top_n: int = 20) -> List[Tuple[str, float]]:
        """ä½¿ç”¨TF-IDFæå–å…³é”®è¯"""
        if not texts or not self.tfidf_vectorizer:
            return []
        
        try:
            tfidf_matrix = self.tfidf_vectorizer.transform(texts)
            feature_names = self.tfidf_vectorizer.get_feature_names_out()
            
            # è®¡ç®—å¹³å‡TF-IDFåˆ†æ•°
            avg_tfidf = np.mean(tfidf_matrix.toarray(), axis=0)
            
            # è·å–top_nä¸ªå…³é”®è¯
            top_indices = avg_tfidf.argsort()[-top_n:][::-1]
            
            keywords = [(feature_names[i], avg_tfidf[i]) for i in top_indices]
            return keywords
        except:
            return []